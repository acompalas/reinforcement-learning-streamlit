# Reinforcement Learning Foundations  
*Streamlit Blog for Digestible RL Concepts*  

This section of the blog covers the **core foundations of Reinforcement Learning**, from multi-armed bandits to advanced policy optimization and model-based methods.  

## Topics Covered  

- **Multi-Armed Bandits**  
  - Action values  
  - Explore–exploit tradeoff (ε-greedy, UCB, Thompson Sampling)  

- **Markov Decision Processes (MDPs)**  
  - States, actions, rewards, transitions  
  - Bellman equations (value and Q functions)  
  - Solution methods:  
    - Dynamic Programming (Policy Iteration, Value Iteration)  
    - Monte Carlo Methods (Prediction & Control)  

- **Temporal Difference Learning**  
  - TD(0) prediction  
  - SARSA (on-policy)  
  - Q-Learning (off-policy)  
  - Expected SARSA  

- **Deep Q-Networks (DQN)**  
  - Replay buffer and target networks  
  - Stabilizing deep value learning  

- **Policy Gradients**  
  - REINFORCE algorithm  
  - Variance reduction with baselines  
  - Discrete vs continuous action spaces  

- **Actor–Critic Methods**  
  - Combining value and policy learning  
  - Advantage Actor–Critic (A2C, A3C)  

- **Advanced Policy Optimization**  
  - **PPO (Proximal Policy Optimization)**  
  - **TRPO (Trust Region Policy Optimization)**  
  - **DDPG & TD3 (Deterministic Actor–Critic for continuous control)**  
  - **SAC (Soft Actor–Critic with entropy regularization)**  

- **Model-Based RL Methods**  
  - Model-free vs model-based framing  
  - Dyna-Q  
  - World Models  
  - Planning and search-based methods (e.g. MuZero)  

- **Extensions** *(shorter, elective topics)*  
  - Eligibility Traces (TD(λ))  
  - Exploration strategies (intrinsic motivation, curiosity)  
  - Hierarchical RL (options, subgoals)  
  - Multi-Agent RL (cooperative & competitive settings)  
